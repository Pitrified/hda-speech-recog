% !TEX root = report.tex

\section{Concluding Remarks}
\label{sec:conclusions}

% A gripping conclusion

The task of identifing a command in a single audio sample has been successfully
solved, with varying degree of accuracy, by every architecture tested.

Further works might focus on different types of attention, and the performance
of SimpleNet and AreaNet should be evaluated on standard image classification
datasets, such as ImageNet.
%
Moreover, the more complex datasets might provide insight to whether the
proposed attention mechanism is capable of learning to pinpoint more complex
features that can be considered useful.
%
Having proved the 
effectiveness of SimpleNets
use them with transfer learning.

% TODO: after all learning attention weights is an *unsupervised* task
% as we do not know which regions are useful
% using object bounding boxes might be interesting first train the general
% feature extractor on loss measured against the boxes
% 0000000000
% 0001111000
% 0001111000
% 0000000000
% then use that to weigh the images

% TODO dighe che vuoi vedere se si rivela utile

Regarding the methodology used, a definite improvement would be using existing
frameworks for hyper-parameter tuning, such as Hypertune, readily available
with Keras Tuner library. A very interesting approach is Hyperband, that trains
every model built from the hyper-parameter grid for a few epochs, then prunes
the set of models, only keeping the best performing ones.

% TODO also much better preprocessing pipeline in ImageNetGenerator

The methodical hyper-parameter analysis done while writing the report was very
informative, and while the plots confirmed what I believed to be the best
combinations, they gave me a couple of ideas regarding some explorations that I
had missed.
