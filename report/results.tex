% !TEX root = report.tex

\section{Results}
\label{sec:results}
% Dazzling numerical results

\subsection{Performance measure}

% \subsubsection{F-Score}
% TODO: what is the F-Score

The F-Score is a metric that combines the values of precision and recall to
provide an estimate of the quality of the predictions made on a test dataset.
Precision is the fraction of relevant data among the selected instances,
the number of true positives over all the samples selected
(a high precision score means that there are few false positives).
% TODO how confident you are that a selected is really in that class.
Recall is the fraction of selected instances among all the relevant samples,
the number of true positives over all the true samples
(a high recall score means that there are few false negatives).
The F-Score is obtained as the harmonic mean of precision and recall.

% Confusion matrix
Another tool used to visually show the performance of a model
is the confusion matrix, that is closely related to the F-Score.
The predicted labels are listed on the x-axis,
the real labels are listed on the y-axis.
% On the x-axis the predicted labels are listed,
% on the y-axis the real labels are listed.
The confusion matrix contains
the number of occurences
for each combination of predicted/real labels.
The main diagonal contains the correctly identified samples.
Along the rows there are the false negatives for a label,
and along the columns there are the false positives for a label.

% \subsubsection{Graphs}

Throughout this section, several groups of graphs will be shown, that present
at once the F-score value for combinations of up to four hyper-parameters.
For example, say there is a graph 
with title 
``paramA: valueA, paramB: [valuesB]
grouped by paramC: [valuesC]
grouped by paramD: [valuesD]''.
This graph is part of a group of graphs, where the value of the first parameter
(valueA) is fixed for each sub-graph.
The other three parameters vary within the sub-graph:
% each packet of columns shows the variation of the parameter B,
% whose values are shown in the legend.
within each group of column the parameter B is changing,
whose values are shown in the legend.
% The groups of columns show the variation of the parameter C,
% whose values are shown as labels of the x axis.
% Each super group of columns show the variation of the parameter D,
% whose values are shown between parenthesis as labels of the x axis.
The other two parameters vary for each group of columns:
the values are indicated in the label of the x axis as ``valueC (valueD)''.
The other hyper-parameters are averaged for each column.
% , within reason:
% Average the others within reason (eg only one type of task).
The height of the column shows the mean F-score value for that
combination of parameters,
the black line indicates the min/max values of F-score and the
blue line tells the standard deviation around the mean.

\subsection{Hyper-parameter analysis: CNN}

% \subsection{Hyper-parameter analysis}
% Hypa comparison
% First specific hypas

A total of $4664$ experiment were performed for the convolutional architecture.

\subsection{Hyper-parameter analysis: Transfer}

A total of $130$ experiment were performed for the transfer learning approach.

\subsection{Hyper-parameter analysis: Attention}

A total of $1389$ experiment were performed for the LSTM+attention model.

\subsubsection{Architecture performance comparison}

\fig{fig:att_dropout_conv_query_dense} shows F-score values for 
variation of 
the width of the dense classifier ($32$ or $64$),
the dropout rate ($0.2$ or $0$) after the initial convolutional layers,
the number of initial convolutional layers ($1$ or $2$)
and
the query style ($01$ and $05$ pick a single LSTM vector to compute the scores,
$02$ and $03$ use convolutional layers connected to the LSTM outputs to extract
the scores and $04$  use convolutional layers connected to the spectrograms).
The results are very close but two conclusions can be tentatively reached:

\begin{itemize}
    \item The query type does not influence
% TODO influence suona male
the results: within each packet of columns the values are very close to each
other, within one standard deviation.
    \item The combination of dense 02 ($64$ units), conv 01 ($1$ layer) and dropout 01 ($0.2$)
seems to be consistently better than the average.
The best values might be found with other combinations of parameters, but those
combinations are less robust, as indicated by the higher standard deviation,
and the quality of the model is less assured when training that combination for
new datasets.
\end{itemize}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{Fscore_att2_dropout__conv__query__dense.pdf}
    \caption{F-score for varying
        dense classifier width,
        query type,
        convolution type,
        dropout.
        Averaged on 20 and 35 words task, solved by the LSTM+attention architecture.
        % TODO should also average on non ugmented dataset
        }%
    \label{fig:att_dropout_conv_query_dense}
\end{figure*}

\subsubsection{Training parameter performance comparison}

% Optimizer is fixed

\fig{fig:att_epoch_batch_lr_words} shows F-score values for 
variation of 
the number of training epochs ($2$, $4$, $15$ and $30$),
the batch size ($16$ or $32$),
and the learning rate
when the LSTM+attention model solves the task \texttt{k1}
on non-augmented datasets.
Two main conclusions can be reached:
\begin{itemize}
    \item
        The training gets more stable as the epoch number increases, and when
        using a larger batch size. For $30$ and $32$ respectively the results
        are quite consistent across the different learning rate types.
    \item
        The learning rate schedule \texttt{clr\_tri2\_04} (a cyclic learning
        rate with triangular 2 shape), when using enough epochs and a large
        batch size, is the one that performs the best.
\end{itemize}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{Fscore_att2_epoch__batch__lr__words.pdf}
    \caption{F-score for varying
        epoch num,
        batch size,
        learning rate type,
        words type.
        Averaged on non-augmented datasets.
        Solved by the LSTM+attention architecture.
        }%
    \label{fig:att_epoch_batch_lr_words}
\end{figure*}

\subsection{AreaNet and SimpleNet performance}

So good!

\subsection{Data augmentation performance}

% TODO: type 01 aug was tested but too long
% TODO: combine 01 and 15 for the final augmentation

As explained in \secref{sec:data_augmentation}, several types of augmentation
were performed on the dataset.
% The bulk of the investigation was to 
% determine which kind of augmentation performed better.
Each type of augmentation is based on one set of spectrogram parameters,
and there are four styles of warp based aumentation:
the landmarks are shifted
1) along both axes of the spectrogram,
2) only along the time axis,
3) only along the frequency axis,
4) along no axis, to provide a reference measure.
Three different spectrogram parameters and two different warping parameters
were tested.
\tab{tab:aug_values}, in the Appendix, shows the specific values used.
For the ``big'' type, $3$ landmarks are shifted by at most $5$ units,
while for the ``small'' type, $4$ landmarks are shifted by at most $2$ units.
% The main differences in augmentation can be seen
% between the ``big'' and ``small'' types.
This leads to the largest difference in augmentation performance:
\tab{tab:att_dataset_result_augmentation} shows that the ``small'' augmentation
type leads to an increase in F-Score value from $0.015$ to $0.03$ relative to
the ``big'' augmentation, both when training the convolutional architecture
(\secref{sec:convolutional_arch}) on a 4-words task, and when using the
LSTM+attention (\secref{sec:attention_model}) on a 10-words task.
This makes sense: the spectrograms used for augmentation have shape $(64, 64)$,
so moving a landmark by $5$ changes too sharply the image.
A more uniform augmentation, with more landmarks but with 
gentler
% TODO what
warping leads to better results.
For the convolutional architecture the mean F-Score value for the 
\texttt{f1} 4-words task increases from $0.939$ to $0.960$,
while for the LSTM+attention architecture, data augmentation does not increase
the performance of the model.
However, the speed with which a high performance is achieved is influenced
by the presence of augmented data.
As shown in \tab{tab:augmentation_learning_speed} and in
\fig{fig:augmentation_learning_speed}, 
peak performance for the model is reached after just $4$ epochs of training,
compared to $10$ to $15$ epochs when using non-augmented data.

\begin{table}[t!]
    \centering
    % \caption{Comparison between regular and augmented datasets:}
    \caption{Comparison between different types of augmentation,
    for the 10-words task solved by LSTM+attention architecture and a
    4-words task solved by the convolutional architecture.
    A reference non-augmented, well performing dataset is added.}
    \label{tab:att_dataset_result_augmentation}
    \begin{tabular}{|cc|c|}
        \hline
        Aug ID & Spec+Type & F-Score \\
        \hline
        \hline
        % CNN & & (4 words)  \\
        % \multicolumn{2}{|c}{CNN} & (4 words) \\
        \multicolumn{3}{|c|}{CNN (4-words task)} \\
        % \hline
        % \hline
        % Aug ID & Spec+Type & F-Score \\
        \hline
        mel04 & None & $0.939 \pm 0.017$ \\
        02,03,04 & mel\_02+big   & $0.940 \pm 0.014$ \\
        06,07,08 & mel\_01+big   & $0.945 \pm 0.015$ \\
        10,11,12 & mel\_03+big   & $0.946 \pm 0.014$ \\
        14,15,16 & mel\_03+small & $0.960 \pm 0.008$ \\
        \hline
        \hline
        % \multicolumn{2}{c}{Multi-column}
        % LSTM+att & & (10 words) \\
        \multicolumn{3}{|c|}{LSTM+att (10-words task)} \\
        \hline
        % mel01   &  0.954 & 0.0073 \\
        mel04   &  None & $0.963 \pm 0.0126$ \\
        % mel05   & None &  $0.954 \pm 0.0184$ \\
        % mela1   &  0.958 & 0.0155 \\
        02,03,04 & mel\_02+big   &  $0.929 \pm 0.0362$ \\
        06,07,08 & mel\_01+big   &  $0.939 \pm 0.0392$ \\
        10,11,12 & mel\_03+big   &  $0.932 \pm 0.0852$ \\
        14,15,16 & mel\_03+small &  $0.964 \pm 0.0129$ \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[t!]
    \centering
    \caption{Comparison of learning speed when training the LSTM+attention
    model on the $10$-words task \texttt{k1}.}
    \label{tab:augmentation_learning_speed}
    \begin{tabular}{|c|c|c|}
        \hline
        Epoch num & Regular & Augmented \\
        \hline
        2 & $0.923 \pm 0.034$ & $0.941 \pm 0.019$ \\
        4 & $0.948 \pm 0.016$ & $0.958 \pm 0.011$ \\
        15 & $0.959 \pm 0.013$ & $0.962 \pm 0.014$ \\
        \hline
    \end{tabular}
\end{table}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{comparison_augmentation.pdf}
    \caption{Comparison of the learning speed on regular vs. augmented data.}%
    \label{fig:augmentation_learning_speed}
\end{figure}

In \tab{tab:att_augmentation_comparison} the different types of
augmentation, along different axis, are compared:
no particular type emerges as better than the others.

\begin{table}[t!]
    \centering
    \caption{Comparison between different styles of augmentation,
    for the 10-words task solved by LSTM+attention architecture and a
    4-words task solved by the convolutional architecture.}
    \label{tab:att_augmentation_comparison}
    \begin{tabular}{|c|c|}
        \hline
        Augmentation type & F-Score \\
        \hline
        \hline
        \multicolumn{2}{|c|}{LSTM+attention (10 words task)} \\
        \hline
        Both axis & $0.9326 \pm 0.0615$ \\
        Time      & $0.9448 \pm 0.0323$ \\
        Frequency & $0.9355 \pm 0.0383$ \\
        None      & $0.9311 \pm 0.0794$ \\
        \hline
        \hline
        \multicolumn{2}{|c|}{CNN (4 words task)} \\
        \hline
        Both axis & $0.945 \pm 0.016$ \\
        Time      & $0.946 \pm 0.014$ \\
        Frequency & $0.944 \pm 0.014$ \\
        None      & $0.940 \pm 0.016$ \\
        \hline
    \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%

\subsection{Extract loudest section performance}

As an additional preprocessing step, the loudest $0.5$s section of the audio
signal was selected.
It might seem surprising, but both the LSTM+attention and the AreaNet
architectures performed better on the full audio samples, 
as shown in \tab{tab:comparison_loud_section}.
This however could be explained by the fact that those are \textit{attention}
architectures, so by design they will identify the relevant portion of the data.
By only feeding important data to the networks, the attention mechanism has more
difficulty to learn which sections to select, because all of them are useful.

% TODO
% Check if this is the case with ConvNets
% SIM are ConvNets tho, and still LS does not help

\begin{table}[t!]
    \centering
    \caption{Comparison of the original dataset and the dataset that only
    includes the loudest section of the audio samples.}
    \label{tab:comparison_loud_section}
    \begin{tabular}{|c|c|c|}
        \hline
        & LTnum & LTnumLS \\
        \hline
        CNN      & $0.967 \pm 0.000$ & $0.952 \pm 0.003$ \\
        LSTM+att & $0.969 \pm 0.012$ & $0.953 \pm 0.012$ \\
        AreaNet  & $0.977 \pm 0.011$ & $0.962 \pm 0.023$ \\
        \hline
        % & LSTM+att & AreaNet(s) \\
        % LTnum   & $0.969 \pm 0.012$ & $0.977 \pm 0.011$ \\
        % LTnumLS & $0.953 \pm 0.012$ & $0.962 \pm 0.023$ \\
    \end{tabular}
\end{table}

\subsection{Architecture comparison}

Show best and top 5 average

Then aggregate table for cross architecture

Pick the best 5 models per category for each task

Compare: CNN, Dense, Xception, EfficientB047, LSTM+attention, AreaNet, SimpleNet,
VerticalAreaNet on num, all, LTnum, LTall, numLS, allLS

Not every combination of everything

Show confusion matrices, speak about similar sounding words, note how AreaNet
does not miss them

% \input{megacomparison_manual}
\input{megacomparison_auto}

\subsection{FSDD performance}

Test all on that, MAYBE merge with architecture comparision

\subsection{Attention weights}

\subsubsection{Attention model}

% Show attention weights for Att model

The LSTM+attention model described in \secref{sec:attention_model} computes a query
vector that is used to weigh the LSTM outputs. Showing the attention weights
can help understand which parts of the recording were relevant for the
classification.
\fig{fig:attention_weights_standard} shows the spectrograms, the attention
weigths and the predictions for three sample words.
Indeed, the attention weights show which some sections of the data is more important
and is used to extract information from the signal.

% ATT_ct02_dr01_ks01_lu01_qt05_dw01_opa1_lr03_bs02_en02_dsaug07_wLTnum_LTnum_train_data
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{ATT_ct02_dr01_ks01_lu01_qt05_dw01_opa1_lr03_bs02_en02_dsaug07_wLTnum_LTnum_train_data.pdf}
    \caption{Spectrograms, attention weights and predictions for three sample words.
    Notice how the attention weights correctly selected the interesting part of
    the ``eight'' spectrogram, avoiding the noise in the latter part.
    For ``\_other\_ltts'', which corresponds to a random audio snippet from the LibriTTS
    dataset, the attention weights still selected the section where a word is spoken,
    and, with some small uncertainty, the word is indeed recognized as ``other''.}%
    \label{fig:attention_weights_standard}
\end{figure*}

\subsection{Stream predictions}

Compare normal vs loud

Attention vs Simple vs AreaNet vs VerticalAreaNet

Compare inference time, model size
