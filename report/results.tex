% !TEX root = report.tex

\section{Results}
\label{sec:results}
% Dazzling numerical results

\subsection{Performance measure}

\subsubsection{F-Score}

TODO: what is the F-score

Confusion matrix

\subsubsection{Graphs}

Throughout this section, several groups of graphs will be shown, that present
at once the F-score value for combinations of up to four hyper-parameters.
For example, say there is a graph 
with title 
``paramA: valueA, paramB: [valuesB]
grouped by paramC: [valuesC]
grouped by paramD: [valuesD]''.
This graph is part of a group of graphs, where the value of the first parameter
(valueA) is fixed for each sub-graph.
The other three parameters vary within the sub-graph:
% each packet of columns shows the variation of the parameter B,
% whose values are shown in the legend.
within each group of column the parameter B is changing,
whose values are shown in the legend.
% The groups of columns show the variation of the parameter C,
% whose values are shown as labels of the x axis.
% Each super group of columns show the variation of the parameter D,
% whose values are shown between parenthesis as labels of the x axis.
The other two parameters vary for each group of columns:
the values are indicated in the label of the x axis as ``valueC (valueD)''.
The other hyper-parameters are averaged for each column.
% , within reason:
% Average the others within reason (eg only one type of task).

The height of the column shows the mean F-score value for that
combination of parameters,
the black line indicates the min/max values of F-score and the
blue line tells the standard deviation around the mean.

\subsection{Hyper-parameter analysis: CNN}

% \subsection{Hyper-parameter analysis}
% Hypa comparison
% First specific hypas

A total of $4664$ experiment were performed for the convolutional architecture.

\subsection{Hyper-parameter analysis: Transfer}

A total of $130$ experiment were performed for the transfer learning approach.

\subsection{Hyper-parameter analysis: Attention}

A total of $1389$ experiment were performed for the LSTM+attention model.

\subsubsection{Architecture performance comparison}

\fig{fig:dropout_conv_query_dense} shows F-score values for 
variation of 
the width of the dense classifier ($32$ or $64$),
the dropout rate ($0.2$ or $0$) after the initial convolutional layers,
the number of initial convolutional layers ($1$ or $2$)
and
the query style ($01$ and $05$ pick a single LSTM vector to compute the scores,
$02$ and $03$ use convolutional layers connected to the LSTM outputs to extract
the scores and $04$  use convolutional layers connected to the spectrograms).
The results are very close but two conclusions can be tentatively reached:

\begin{itemize}
    \item The query type does not influence
% TODO influence suona male
the results: within each packet of columns the values are very close to each
other, within one standard deviation.
    \item The combination of dense 02 ($64$ units), conv 01 ($1$ layer) and dropout 01 ($0.2$)
seems to be consistently better than the average.
The best values might be found with other combinations of parameters, but those
combinations are less robust, as indicated by the higher standard deviation,
and the quality of the model is less assured when training that combination for
new datasets.
\end{itemize}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{Fscore_att2_dropout__conv__query__dense.pdf}
    \caption{F-score for varying
        dense classifier width,
        query type,
        convolution type,
        dropout.
        Averaged on 20 and 35 words task, solved by the LSTM+attention architecture.
        % TODO should also average on non ugmented dataset
        }%
    \label{fig:dropout_conv_query_dense}
\end{figure*}

\subsubsection{Training parameter performance comparison}

Group by (lr, batch, epoch)
Optimizer is fixed
TODO: Almost ready, finish last experiments

\subsection{AreaNet and SimpleNet performance}

So good!

\subsection{Data augmentation performance}

% TODO: type 01 aug was tested but too long
% TODO: combine 01 and 15 for the final augmentation

As explained in \secref{sec:data_augmentation}, several types of augmentation
were performed on the dataset.
% The bulk of the investigation was to 
% determine which kind of augmentation performed better.
Each type of augmentation is based on one set of spectrogram parameters,
and there are four styles of warp based aumentation:
the landmarks are shifted
1) along both axes of the spectrogram,
2) only along the time axis,
3) only along the frequency axis,
4) along no axis, to provide a reference measure.
Three different spectrogram parameters and two different warping parameters
were tested.
\tab{tab:aug_values}, in the Appendix, shows the specific values used.
For the ``big'' type, $3$ landmarks are shifted by at most $5$ units,
while for the ``small'' type, $4$ landmarks are shifted by at most $2$ units.
% The main differences in augmentation can be seen
% between the ``big'' and ``small'' types.
This leads to the largest difference in augmentation performance:
\tab{tab:att_dataset_result_augmentation} shows that the ``small'' augmentation
type leads to an increase in F-Score value from $0.15$ to $0.3$, relative to
the ``big'' augmentation, both when training the convolutional architecture
(\secref{sec:convolutional_arch}) on a 4-words task and when using the
LSTM+attention (\secref{sec:attention_model}) on a 10-words task.
This makes sense: the spectrograms used for augmentation have shape 
$(64, 64)$,
so moving a landmark by $5$ changes too sharply the image.
A more uniform augmentation, with more landmarks but with 
gentler
% TODO what
warping leads to better results.
For the LSTM+attention architecture, data augmentation does not increase
the performance of the model,
whereas for the convolutional architecture the mean F-Score value for the 
\texttt{f1} 4-words task increases from $0.939$ to $0.960$.

In \tab{tab:att_augmentation_comparison} the different types of
augmentation are compared.
No particular type emerges as better than the others.

\begin{table}[t!]
    \centering
    % \caption{Comparison between regular and augmented datasets:}
    \caption{Comparison between different types of augmentation,
    for the 10-words task solved by LSTM+attention architecture and a
    4-words task solved by the convolutional architecture.
    A reference non-augmented, well performing dataset is added.}
    \label{tab:att_dataset_result_augmentation}
    \begin{tabular}{|cc|c|}
        \hline
        Aug ID & Spec+Type & F-Score \\
        \hline
        \hline
        % \multicolumn{2}{c}{Multi-column}
        % LSTM+att & & (10 words) \\
        \multicolumn{3}{|c|}{LSTM+att (10-words task)} \\
        \hline
        % mel01   &  0.954 & 0.0073 \\
        mel04   &  None & $0.963 \pm 0.0126$ \\
        % mel05   & None &  $0.954 \pm 0.0184$ \\
        % mela1   &  0.958 & 0.0155 \\
        02,03,04 & mel\_02+big   &  $0.929 \pm 0.0362$ \\
        06,07,08 & mel\_01+big   &  $0.939 \pm 0.0392$ \\
        10,11,12 & mel\_03+big   &  $0.932 \pm 0.0852$ \\
        14,15,16 & mel\_03+small &  $0.964 \pm 0.0129$ \\
        \hline
        \hline
        % CNN & & (4 words)  \\
        % \multicolumn{2}{|c}{CNN} & (4 words) \\
        \multicolumn{3}{|c|}{CNN (4-words task)} \\
        % \hline
        % \hline
        % Aug ID & Spec+Type & F-Score \\
        \hline
        mel04 & None & $0.939 \pm 0.017$ \\
        02,03,04 & mel\_02+big   & $0.940 \pm 0.014$ \\
        06,07,08 & mel\_01+big   & $0.945 \pm 0.015$ \\
        10,11,12 & mel\_03+big   & $0.946 \pm 0.014$ \\
        14,15,16 & mel\_03+small & $0.960 \pm 0.008$ \\
        \hline
    \end{tabular}
\end{table}

\begin{table}[t!]
    \centering
    \caption{Comparison between different styles of augmentation,
    for the 10-words task solved by LSTM+attention architecture and a
    4-words task solved by the convolutional architecture.}
    \label{tab:att_augmentation_comparison}
    \begin{tabular}{|c|c|}
        \hline
        Augmentation type & F-Score \\
        \hline
        \hline
        \multicolumn{2}{|c|}{LSTM+attention (10 words task)} \\
        \hline
        Both axis & $0.9326 \pm 0.0615$ \\
        Time      & $0.9448 \pm 0.0323$ \\
        Frequency & $0.9355 \pm 0.0383$ \\
        None      & $0.9311 \pm 0.0794$ \\
        \hline
        \hline
        \multicolumn{2}{|c|}{CNN (4 words task)} \\
        \hline
        Both axis & $0.945 \pm 0.016$ \\
        Time      & $0.946 \pm 0.014$ \\
        Frequency & $0.944 \pm 0.014$ \\
        None      & $0.940 \pm 0.016$ \\
        \hline
    \end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%

\subsection{Extract loudest section performance}

As an additional preprocessing step, the loudest $0.5$s section of the audio
signal was selected.
It might seem surprising, but both the LSTM+attention and the AreaNet
architectures performed better on the full audio samples, 
as shown in \tab{tab:comparison_loud_section}.
This however could be explained by the fact that those are \textit{attention}
architectures, so by design they will identify the relevant portion of the data.
By only feeding important data to the networks, the attention mechanism has more
difficulty to learn which sections to select, because all of them are useful.

% TODO it makes sense that the attention models are LESS performing on reduced
% input, as all of ti will be useful. Check if this is the case with ConvNets
% SIM are ConvNets tho, and still LS suks

\begin{table}[t!]
    \centering
    \caption{Comparison of the original dataset and the dataset that only
    includes the loudest section of the audio samples.}
    \label{tab:comparison_loud_section}
    \begin{tabular}{|c|c|c|}
        \hline
        & LTnum & LTnumLS \\
        \hline
        CNN      & $0.967 \pm 0.000$ & $0.952 \pm 0.003$ \\
        LSTM+att & $0.969 \pm 0.012$ & $0.953 \pm 0.012$ \\
        AreaNet  & $0.977 \pm 0.011$ & $0.962 \pm 0.023$ \\
        \hline
        % & LSTM+att & AreaNet(s) \\
        % LTnum   & $0.969 \pm 0.012$ & $0.977 \pm 0.011$ \\
        % LTnumLS & $0.953 \pm 0.012$ & $0.962 \pm 0.023$ \\
    \end{tabular}
\end{table}

\subsection{FSDD performance}

Test all on that.

\subsection{Architecture comparison}

Show best and top 5 average

Then aggregate table for cross architecture

Pick the best 5 models per category for each task

Compare: CNN, Dense, Xception, EfficientB047, LSTM+attention, AreaNet, SimpleNet,
VerticalAreaNet on num, all, LTnum, LTall, numLS, allLS

Not every combination of everything

Show confusion matrices, speak about similar sounding words, note how AreaNet
does not miss them

\subsection{Attention weights}

\subsubsection{Attention model}

% Show attention weights for Att model

The LSTM+attention model described in \secref{sec:attention_model} computes a query
vector that is used to weigh the LSTM outputs. Showing the attention weights
can help understand which parts of the recording were relevant for the
classification.
\fig{fig:attention_weights_standard} shows the spectrograms, the attention
weigths and the predictions for three sample words.
Indeed, the attention weights show which some sections of the data is more important
and is used to extract information from the signal.

% ATT_ct02_dr01_ks01_lu01_qt05_dw01_opa1_lr03_bs02_en02_dsaug07_wLTnum_LTnum_train_data
\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{ATT_ct02_dr01_ks01_lu01_qt05_dw01_opa1_lr03_bs02_en02_dsaug07_wLTnum_LTnum_train_data.pdf}
    \caption{Spectrograms, attention weights and predictions for three sample words.
    Notice how the attention weights correctly selected the interesting part of
    the ``eight'' spectrogram, avoiding the noise in the latter part.
    For ``\_other\_ltts'', which corresponds to a random audio snippet from the LibriTTS
    dataset, the attention weights still selected the section where a word is spoken,
    and, with some small uncertainty, the word is indeed recognized as ``other''.}%
    \label{fig:attention_weights_standard}
\end{figure*}

\subsubsection{AreaNet}

The driving idea behind AreaNet is to score the original spectrogram with a
grid of attention scores. Two examples of the weights computed by AreaNet are
shown in \fig{fig:attention_weights_area}, and it can be seen that the model
focuses on the lowest frequencies aligned with the portion of utterance where
the word is actually being spoken, correctly extracting the relevant portion 
of the spectrograms.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{AAN_opa1_lr03_bs32_en15_dsaug14_wLTnum_0001.pdf}
    \caption{AreaNet computes a $8 \times 8$ grid that is used to weigh the
    original spectrogram.}%
    \label{fig:attention_weights_area}
\end{figure}

TODO: Show VerticalAreaNet

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{VAN_opa1_lr04_bs32_en15_dsaug14_wLTall_0001.pdf}
    \caption{VerticalAreaNet computes a $1 \times 16$ grid that is used to
        weigh the original 
    spectrogram.}%
    \label{fig:attention_weights_vertical}
\end{figure}
\subsection{Stream predictions}

Compare normal vs loud

Attention vs Simple vs AreaNet vs VerticalAreaNet

Compare inference time, model size
